{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python | Word Embedding using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate word vectors using Word2Vec \n",
    "  \n",
    "# importing all necessary modules \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "  \n",
    "#  Reads ‘alice.txt’ file \n",
    "txt = open('data/alice_in_wonderland.txt', 'r', encoding='utf-8').read()\n",
    "  \n",
    "# Replaces escape character with space \n",
    "f = txt.replace(\"\\n\", \" \") \n",
    "  \n",
    "data = []\n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW model\n",
    "\n",
    "<b>CBOW</b> (Continuous Bag of Words) : CBOW model predicts the current word given context words within specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent current word present at the output layer.\n",
    "\n",
    "![](data/img/cbow-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.9995018\n",
      "Cosine similarity between 'alice' and 'machines' - CBOW :  0.96151334\n"
     ]
    }
   ],
   "source": [
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'alice' \" + \n",
    "               \"and 'wonderland' - CBOW : \", \n",
    "    model1.similarity('alice', 'wonderland')) \n",
    "      \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "                 \"and 'machines' - CBOW : \", \n",
    "      model1.similarity('alice', 'machines')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Gram Model\n",
    "\n",
    "<b>Skip Gram :</b> Skip gram predicts the surrounding context words within specific window given current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer.\n",
    "\n",
    "![](data/img/skip_gram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.8744346\n",
      "Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.8473991\n"
     ]
    }
   ],
   "source": [
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 1) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "          \"and 'wonderland' - Skip Gram : \", \n",
    "    model2.similarity('alice', 'wonderland')) \n",
    "      \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "            \"and 'machines' - Skip Gram : \", \n",
    "      model2.similarity('alice', 'machines')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model1.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'project', 'gutenberg', 'ebook', 'of']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in binary format\n",
    "\t\n",
    "model1.save('model/CBOW_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in text format\n",
    "model1.wv.save_word2vec_format('model/CBOW_model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "model = Word2Vec.load('model/CBOW_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.10204594,  0.18292864,  0.87800646, -0.38213748,  0.2672963 ,\n",
       "       -0.32852837,  0.17967647,  0.10020731, -0.27500832,  0.34737298,\n",
       "       -0.40543127,  0.08899716, -0.2713697 , -0.7803048 , -0.67667294,\n",
       "        0.6250244 ,  0.0229868 ,  0.14890417, -0.14119212, -0.04553616,\n",
       "       -0.20357025, -0.03966459, -0.4305795 ,  0.3788454 ,  0.07458749,\n",
       "       -0.2830592 , -0.025791  ,  0.00518946, -0.05386501, -0.35998458,\n",
       "       -0.1914014 , -0.20371711, -0.05937751,  0.05216431,  0.25186336,\n",
       "       -0.1403319 ,  0.6124175 , -0.28204593, -0.19261882,  0.20210662,\n",
       "        0.40035596, -0.31916404,  0.14216146, -0.03328944, -0.22096813,\n",
       "       -0.21119332, -0.08322362,  0.08064889, -0.36361706, -0.20035002,\n",
       "       -0.39023587, -0.0895891 ,  0.29216364,  0.00807368, -0.12038402,\n",
       "        0.130752  , -0.14050558,  0.19581953,  0.01237107, -0.03921397,\n",
       "        0.13041776, -0.29148465, -0.06576138, -0.45159632,  0.05770196,\n",
       "       -0.26694065,  0.5139964 , -0.3004852 ,  0.06345735,  0.06408159,\n",
       "        0.12055822,  0.32152045, -0.24071051,  0.23754805,  0.01786217,\n",
       "       -0.11319338,  0.41646528,  0.09459817, -0.36858904,  0.62304366,\n",
       "       -0.5178726 , -0.5000704 ,  0.46077514,  0.76380455,  0.1089972 ,\n",
       "        0.18539985,  0.29733193,  0.03729437,  0.51836526, -0.01787901,\n",
       "       -0.1962485 , -0.56612194,  0.28524807,  0.06550652, -0.38980594,\n",
       "        0.21533135,  0.0385569 ,  0.19712314,  0.19723013,  0.06096794],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model['alice'].shape)\n",
    "model['alice'] - model['wonderland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7999394\n",
      "0.99836755\n",
      "0.9995018\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('alice', 'duck'))\n",
    "print(model.similarity('alice', 'hair'))\n",
    "print(model.similarity('alice', 'wonderland'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('large', 0.9994753003120422), ('my', 0.9994434714317322), ('say', 0.9994432926177979), ('me', 0.9994370341300964), (')', 0.9994361400604248)]\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive=['alice', 'wonderland'], negative=['rabbit'], topn=5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('picture', 0.22679075598716736), ('contempt', 0.21095998585224152), ('backs', 0.2065446376800537), ('jack-in-the-box', 0.19726744294166565), ('reasons', 0.191685751080513)]\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive=['alice'], negative=['wonderland'], topn=5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the text model\n",
    "filename = 'model/CBOW_model.txt'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995018"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('alice','wonderland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
